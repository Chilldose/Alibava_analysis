"""This file contains functions and classes which can be classified as utilitie
functions for a more general purpose. Furthermore, these functions are for
python analysis of ALIBAVA files."""
# pylint: disable=C0103,R1710,R0903

import logging
import logging.config
import os
import struct
import sys
from importlib import import_module
import numpy as np
import yaml
# COMMENT: tqdm and h5py are both missing in requirements
import h5py
import yaml
from tqdm import tqdm
from six.moves import cPickle as pickle  # for performance
import matplotlib.pyplot as plt
from matplotlib.backends.backend_pdf import PdfPages

# COMMENT: The python way of doing things is always, as simple as possible. If you
# really want to have differen logger types lets do it this way, as it is suggested it a multitude of
# examples

def init_logger(path='logger.yml', default_level=logging.INFO, env_key='LOG_CFG'):
    """Loads a logger file and initiates logger"""
    value = os.getenv(env_key, None)
    if value:
        path = value
    if os.path.exists(os.path.normpath(path)):
        with open(path, 'rt') as f:
            config = yaml.safe_load(f.read())
        logging.config.dictConfig(config)
    else:
        logging.basicConfig(level=default_level)



def manage_logger(logger, level=logging.DEBUG):
    """Sets output level of logging object and checks if object has a
    StreamHandler"""
    logger.setLevel(level)
    if logger.hasHandlers() is False:
        format_string = '%(asctime)s - %(levelname)s - %(name)s - %(message)s'
        formatter = logging.Formatter(format_string)
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)

LOG = logging.getLogger("utilities")
#manage_logger(LOG)

def load_plugins(valid_plugins):
    # Load all measurement functions
    # install_directory = os.getcwd() # Obtain the install path of this module
    all_plugins = {}
    all_measurement_functions = os.listdir("./analysis_classes/")
    all_measurement_functions = list(set([modules.split(".")[0] for modules in all_measurement_functions]))

    for modules in all_measurement_functions:  # import all modules from all files in the plugins folder
        to_add = import_module("analysis_classes." + modules)
        names = dir(to_add) # get all modules
        for plugin in valid_plugins:
            if plugin in names:
                all_plugins.update({plugin: to_add})
    return all_plugins




def create_dictionary(file, filepath=os.getcwd()):
    """Creates a dictionary with all values written in the file using yaml"""
    file_string = os.path.join(filepath, file)

    with open(file_string, "r") as yfile:
        dic = yaml.safe_load(yfile)
        return dic

def import_h5(*paths):
    """
    This functions imports hdf5 files generated by ALIBAVA.
    If you pass several pathes, then you get list of objects back, containing the data respectively
    :param pathes: pathes to the datafiles which should be imported
    :return: list
    """
    # COMMENT: That should be done one level above. the func should just import 1 hdf5 file
                # and return the same type as the read_binary func
    # Check if a list was passed
    if isinstance(paths[0], list):
        paths = paths[0]

    # First check if pathes exist and if so import
    loaded_files = []
    try:
        for path in tqdm(paths, desc= "Loading files:"):
            if os.path.exists(os.path.normpath(path)):
                # Now import all hdf5 files
                loaded_files.append(h5py.File(os.path.normpath(path), 'r'))
                LOG.debug("Keys: %s", list(loaded_files[-1].keys()))
            else:
                raise Exception('The path {!s} does not exist.'.format(path))
        return loaded_files
    except OSError as err:
        LOG.error("Enountered an OSError: {!s}".format(err))
        return False

def get_xy_data(data, header=0):
    """This functions takes a list of strings, containing a header and xy data,
    return values are 2D np.array of the data and the header lines"""

    np2Darray = np.zeros((len(data)-int(header),2), dtype=np.float32)
    for i, item in enumerate(data):
        if i > header-1:
            list_data = list(map(float,item.split()))
            np2Darray[i-header] = np.array(list_data)
    return np2Darray

def read_binary_Alibava(filepath):
    """Reads binary alibava files"""

    with open(os.path.normpath(filepath), "rb") as f:
        header = f.read(16)
        Starttime = struct.unpack("II", header[0:8])[0]  # Is a uint32
        Runtype = struct.unpack("i", header[8:12])[0]  # int32
        Headerlength = struct.unpack("I", header[12:16])
        header = f.read(Headerlength[0])
        Header = struct.unpack("{}s".format(Headerlength[0]), header)[0].decode("Utf-8")
        Pedestal = np.array(struct.unpack("d" * 256, f.read(8 * 256)), dtype=np.float32)
        Noise = np.array(struct.unpack("d" * 256, f.read(8 * 256)), dtype=np.float32)
        byteorder = sys.byteorder

        # Data Blocks
        # Read all data Blocks
        # Warning Alibava Binary calibration files have no indicatior how many events are really inside the file
        # The eventnumber corresponds to the pulse number -->
        # Readout of files have to be done until end of file is reached
        # and the eventnumber must be calculated --> Advantage: Damaged files can be read as well
        #events = Header.split("|")[1].split(";")[0]
        event_data = []
        events = 0
        eof = False
        #for event in range(int(events)):
        while not eof:
            blockheader = f.read(4)  # should be 0xcafe002
            if blockheader == b'\x02\x00\xfe\xca' or blockheader == b'\xca\xfe\x00\x02':
                events += 1
                blocksize = struct.unpack("I", f.read(4))
                event_data.append(f.read(blocksize[0]))
            else:
                LOG.info("Warning: While reading data Block {}. "
                      "Header was not the 0xcafe0002 it was {!s}".format(events, str(blockheader)))
                if not blockheader:
                    LOG.info("Persumably end of binary file reached. Events read: {}".format(events))
                    eof = True
        # COMMENT: giving variables names like 'dict' that overwrite built-in types/functions is very bad
        dic = {"header": {"noise": Noise,
                            "pedestal": Pedestal,
                          "Attribute:setup": None},
               "events": {"header": Header,
                            "signal": np.zeros((int(events),256), dtype=np.float32),
                            "temperature": np.zeros(int(events), dtype=np.float32),
                            "time": np.zeros(int(events), dtype=np.float32),
                          "clock": np.zeros(int(events), dtype=np.float32)},
                "scan": {"start": Starttime,
                        "end": None,
                         "value": None, # Values of cal files for example. eg. 32 pulses for a charge scan steps should be here
                         "attribute:scan_definition": None}}
        # Disect the header for the correct informations for values
        points = Header.split("|")[1].split(";")
        params = [x.strip("\x00") for x in points]

        # Alibava binary have (unfortunately) a non consistend header format
        # Therefore, we have to distinguish between the two formats --> len(params) = 4 --> Calibration
        # len(params) = 2 --> Eventfile

        if len(params) >= 4: # Cal file
            dic["scan"]["value"] = np.arange(int(params[1]), int(params[2]), int(params[3]))  # aka xdata
        elif len(params) == 2: # Events file
            dic["scan"]["value"] = np.arange(0, int(params[0]),step=1)  # aka xdata

        shift1 = int.from_bytes(b'0xFFFF0000',byteorder=byteorder)
        shift2 = int.from_bytes(b'0xFFFF',byteorder=byteorder)
        # decode data from data Blocks
        for i, event in enumerate(event_data):
            dic["events"]["clock"][i] = struct.unpack("III", event[0:12])[-1]
            coded_time = struct.unpack("I", event[12:16])[0]
            #coded_time = event[12:16]
            ipart = (coded_time & shift1)>>16
            fpart = (np.sign(ipart))*(coded_time & shift2)
            time = 100*ipart+fpart
            dic["events"]["time"][i] = time
            dic["events"]["temperature"][i] = 0.12*struct.unpack("H", event[16:18])[0]-39.8

            # There seems to be garbage data which needs to be cut out
            padding = 18+32
            part1 = list(struct.unpack("h"*128, event[padding:padding+2*128]))
            padding += 2*130+28
            part2 = list(struct.unpack("h" * 128, event[padding:padding + 2*128]))
            part1.extend(part2)
            dic["events"]["signal"][i] = np.array(part1)
            #dict["events"]["signal"][i] =struct.unpack("H"*256, event[18:18+2*256])
            #extra = struct.unpack("d", event[18+2*256:18+2*256+4])[0]

    return dic

def read_file(filepath, binary=False):
    """Just reads a file and returns the content line by line"""
    if os.path.exists(os.path.normpath(filepath)):
        if not binary:
            with open(os.path.normpath(filepath), 'r') as f:
                read_data = f.readlines()
            return read_data

    else:
        LOG.info("No valid path passed: {!s}".format(filepath))
        return None

def clustering(estimator):
    """Does the clustering up to the max cluster number, you just need the
    estimator and its config parameters"""
    return estimator

def count_sub_length(ndarray):
    """This function count the length of sub elements (depth 1) in the ndarray
    and returns an array with the lengthes with the same dimension as ndarray"""
    results = np.zeros(len(ndarray))
    # COMMENT: there is a neat built-in function for this called enumerate
    for i in range(len(ndarray)):
        if len(ndarray[i]) == 1:
            results[i] = len(ndarray[i][0])
    return results


def convert_ADC_to_e(signal, interpolation_function):
    """Gets the signal in ADC, the interpolatrion function and returns an
    array with the interpolated singal in electorns

    :param signal: Singnal array which should be converted, basically the singal from every strip
    :param interpolation_function: the interpolation function
    :return: Returns array with the electron count
    """
    return interpolation_function(np.abs(signal))

def save_all_plots(name, folder, figs=None, dpi=200):
    """
    This function saves all generated plots to a specific folder with the defined name in one pdf
    :param name: Name of output
    :param folder: Output folder
    :param figs: Figures which you want to save to one pdf (leaf empty for all plots) (list)
    :param dpi: image dpi
    :return: None
    """
    # COMMENT: dpi unused???', plt is not defined!
    try:
        pp = PdfPages(os.path.normpath(folder) + "\\" + name + ".pdf")
    except PermissionError:
        print("While overwriting the file {!s} a permission error occured, "
                 "please close file if opened!".format(name + ".pdf"))
        return
    if figs is None:
        figs = [plt.figure(n) for n in plt.get_fignums()]
    for fig in tqdm(figs, desc="Saving plots"):
        #fig = plt.figure()
        fig.set_figheight(9)
        fig.set_figwidth(16)
        fig.savefig(pp, format='pdf')
    pp.close()

class NoStdStreams(object):
    """Surpresses all output of a function when called with with """
    def __init__(self,stdout = None, stderr = None):
        self.devnull = open(os.devnull,'w')
        self._stdout = stdout or self.devnull or sys.stdout
        self._stderr = stderr or self.devnull or sys.stderr

    def __enter__(self):
        self.old_stdout, self.old_stderr = sys.stdout, sys.stderr
        self.old_stdout.flush()
        self.old_stderr.flush()
        sys.stdout, sys.stderr = self._stdout, self._stderr

    def __exit__(self, exc_type, exc_value, traceback):
        self._stdout.flush()
        self._stderr.flush()
        sys.stdout = self.old_stdout
        sys.stderr = self.old_stderr
        self.devnull.close()

def gaussian(x, mu, sig, a):
    """Simple but fast implementation of as gaussian distribution"""
    return a*np.exp(-np.power(x - mu, 2.) / (2. * np.power(sig, 2.)))

def langau_cluster(cls_ind, valid_events_Signal, valid_events_clusters,
                   charge_cal, noise):
    """Calculates the energy of events, clustersize independend"""
    # for size in tqdm(clustersize_list, desc="(langau) Processing clustersize"):
    totalE = np.zeros(len(cls_ind))
    totalNoise = np.zeros(len(cls_ind))
    # Loop over the clustersize to get total deposited energy
    incrementor = 0
    for ind in tqdm(cls_ind, desc="(langau) Processing event"):
        # Signal calculations
        signal_clst_event = np.take(valid_events_Signal[ind],
                                    valid_events_clusters[ind][0])
        totalE[incrementor] = np.sum(convert_ADC_to_e(signal_clst_event,
                                                      charge_cal))

        # Noise Calculations

        # Get the Noise of an event
        noise_clst_event = np.take(noise, valid_events_clusters[ind][0])
        # eError is a list containing electron signal noise
        totalNoise[incrementor] = np.sqrt(np.sum(\
                convert_ADC_to_e(noise_clst_event, charge_cal)))

        incrementor += 1

    preresults = {"signal": totalE, "noise": totalNoise}
    return preresults

def get_size(obj, seen=None):
    """Recursively finds size of objects"""
    size = sys.getsizeof(obj)
    if seen is None:
        seen = set()
    obj_id = id(obj)
    if obj_id in seen:
        return 0
    # Important mark as seen *before* entering recursion to gracefully handle
    # self-referential objects
    seen.add(obj_id)
    if isinstance(obj, dict):
        size += sum([get_size(v, seen) for v in obj.values()])
        size += sum([get_size(k, seen) for k in obj.keys()])
    elif hasattr(obj, '__dict__'):
        size += get_size(obj.__dict__, seen)
    elif hasattr(obj, '__iter__') and not isinstance(obj, (str, bytes, bytearray)):
        size += sum([get_size(i, seen) for i in obj])
    return size

def save_configs(configs, name, path):
    """This function saves the configs of the current run"""
    try:
        yaml.safe_dump(configs, os.path.normpath(path + "\\" + name))
    except OSError as err:
        LOG.error("Failed to save configs.", exc_info=True)


class Bdata:
    """Creates an object which can handle numpy arrays. By passing lables you
    can get the columns of the multidimensional array. Its like a pandas array
    but with way less overhead.
    If you store a Bdata object you can get columns by accessing it via Bdata['label']
    Not passing an argument results in """

    def __init__(self, data = np.array([]), labels = None):

        # Has nothing to do here, this is a Data type not a typical class
        #self.log = logging.getLogger(__class__.__name__)
        #self.log.setLevel(logging.DEBUG)
        #if self.log.hasHandlers() is False:
        #    format_string = '%(asctime)s - %(levelname)s - %(name)s - %(message)s'
        #    formatter = logging.Formatter(format_string)
        #    console_handler = logging.StreamHandler()
        #    console_handler.setFormatter(formatter)
        #    self.log.addHandler(console_handler)

        self.data = data
        self.labels = labels
        self.log = LOG

        if len(self.data) != len(self.labels):
            self.log.warning("Data missmatch!")

    def __getitem__(self, arg=None):
        # COMMENT: else returns 'None' is correct?
        if arg:
            return self.get(arg)

    def __repr__(self):
        return repr(self.data)

    def get(self, label):
        """DOC of function"""
        return self.data[:,self.labels.index(label)]

def save_dict(di_, filename_):
    """DOC of function"""

    with open(os.path.normpath(filename_), 'wb') as f:
        pickle.dump(di_, f)

def load_dict(filename_):
    """DOC of function"""
    with open(os.path.normpath(filename_), 'rb') as f:
        ret_di = pickle.load(f)
    return ret_di


# Here the logger will be initialized!
init_logger(path='logger.yml')


if __name__ == "__main__":
    pass
